import multiprocessing
import os
import struct
import sys
from dataclasses import dataclass
import random

import numpy as np
import capstone
import subprocess
from scipy.optimize import linear_sum_assignment
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import matplotlib as plt
from node2vec import Node2Vec
from node2vec.edges import HadamardEmbedder
from karateclub import Graph2Vec
from sklearn.manifold import TSNE

md = None
name = ""
WORKERS = 8
random.seed(42)
IGNORED_EXTENSIONS = [".py", ".txt", ".npy", ".empty", "_fixed", ".csv"]


def set_disassembler(elf_ident, elf_machine):
    """
    Sets the disassembler based on ELF identifier and machine type.

    :param elf_ident: ELF identifier bytes.
    :param elf_machine: ELF machine type.
    """
    global md
    if elf_ident[5] == 1:
        endianness = capstone.CS_MODE_LITTLE_ENDIAN
    else:
        endianness = capstone.CS_MODE_BIG_ENDIAN
    if elf_ident[4] == 1:
        if elf_machine == 2:
            md = capstone.Cs(capstone.CS_ARCH_SPARC, capstone.CS_MODE_32 + endianness)
        elif elf_machine == 3:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32 + endianness)
        elif elf_machine == 4:
            md = capstone.Cs(capstone.CS_ARCH_M68K, capstone.CS_MODE_32 + endianness)
        elif elf_machine == 8:
            md = capstone.Cs(capstone.CS_ARCH_MIPS, capstone.CS_MODE_MIPS32 + endianness)
        elif elf_machine == 10:
            md = capstone.Cs(capstone.CS_ARCH_MIPS, capstone.CS_MODE_MIPS32 + endianness)
        elif elf_machine == 20:
            md = capstone.Cs(capstone.CS_ARCH_PPC, capstone.CS_MODE_32 + endianness)
        elif elf_machine == 40:
            md = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM + endianness)
    else:
        if elf_machine == 2:
            md = capstone.Cs(capstone.CS_ARCH_SPARC, capstone.CS_MODE_64 + endianness)
        elif elf_machine == 3:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64 + endianness)
        elif elf_machine == 4:
            md = capstone.Cs(capstone.CS_ARCH_M68K, capstone.CS_MODE_64 + endianness)
        elif elf_machine == 8:
            md = capstone.Cs(capstone.CS_ARCH_MIPS, capstone.CS_MODE_MIPS64 + endianness)
        elif elf_machine == 10:
            md = capstone.Cs(capstone.CS_ARCH_MIPS, capstone.CS_MODE_MIPS64 + endianness)
        elif elf_machine == 20:
            md = capstone.Cs(capstone.CS_ARCH_PPC, capstone.CS_MODE_64 + endianness)
        elif elf_machine == 21:
            md = capstone.Cs(capstone.CS_ARCH_PPC, capstone.CS_MODE_64 + endianness)
        elif elf_machine == 40:
            md = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM + endianness)
        elif elf_machine == 62:
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64 + endianness)
    md.skipdata = True


def build_CFG(user_code_info, verbose=False, display_graph=False):
    """
    Builds the Control Flow Graph (CFG) from the user code.

    :param user_code_info: Information about user code sections.
    :param verbose: Whether to print verbose output.
    :param display_graph: Whether to display the graph visually.
    :return: A NetworkX DiGraph representing the CFG.
    """
    global name, md
    lookup_table = {}
    control_flow_graph = None
    if md:
        with open(name, 'rb') as file:
            control_flow_graph = nx.DiGraph()
            max_node = 0
            current_name = 0
            control_flow_graph.add_node(current_name, instructions=[])
            control_flow_instructions_x86 = ['jmp', 'je', 'jne', 'jg', 'jge', 'jl', 'jle', 'ja', 'jae', 'jb', 'jbe',
                                             'call']
            control_flow_instructions_arm = ['b', 'beq', 'bne', 'bgt', 'bge', 'blt', 'ble', 'bhi', 'bhs', 'blo', 'bls',
                                             'bl', 'blx']
            control_flow_instructions_mips = ['j', 'beq', 'beql', 'bne', 'bnel', 'bgtz', 'bgtzl', 'bgez', 'bgezl',
                                              'bltz', 'bltzl', 'blez', 'blezl', 'jal', 'bal',
                                              'beqz', 'bnez']
            return_functions = ['ret', 'bx', 'jr']
            if verbose:
                file_out_name = name.replace(".", "_")
                file_out = open(f'{file_out_name}_final.asm', 'w')
            for uci in user_code_info:
                file.seek(uci[0])
                user_code = file.read(uci[2])

                for instr in md.disasm(user_code, uci[1]):
                    if verbose:
                        file_out.write("0x%x:\t%s\t%s" % (instr.address, instr.mnemonic, instr.op_str))
                        file_out.write('\n')

                    if instr.address in lookup_table and lookup_table[instr.address] == 1:
                        end_for = False
                        for node in control_flow_graph.nodes:
                            if not control_flow_graph.nodes[node]['instructions']:
                                control_flow_graph.nodes[node]['instructions'].append(
                                    {'address': instr.address, 'mnemonic': instr.mnemonic, 'op_str': instr.op_str})
                                current_name = node
                            else:
                                for node_instr in control_flow_graph.nodes[node]['instructions']:
                                    if node_instr['address'] == instr.address:
                                        current_name = node
                                        end_for = True
                                        break
                                if end_for:
                                    break
                        continue

                    if instr.mnemonic in control_flow_instructions_x86 or \
                            instr.mnemonic in control_flow_instructions_arm or \
                            instr.mnemonic in control_flow_instructions_mips:
                        instr_op_str = instr.op_str.replace('#', '')
                        if not (instr_op_str == '0' or not instr_op_str.startswith('0x')):
                            instr_target = int(instr_op_str, base=16)
                            if instr_target in lookup_table and lookup_table[instr_target] == 1:
                                for node in control_flow_graph.nodes:
                                    end_for = False
                                    if not control_flow_graph.nodes[node]['instructions']:
                                        control_flow_graph.add_edge(current_name, node)
                                    else:
                                        for node_instr in control_flow_graph.nodes[node]['instructions']:
                                            if node_instr['address'] == instr_target:
                                                control_flow_graph.add_edge(current_name, node)
                                                current_name = node
                                                end_for = True
                                                break
                                        if end_for:
                                            break
                            else:
                                max_node = max_node if max_node > current_name else current_name
                                max_node += 1
                                prev_name = current_name
                                current_name = max_node
                                control_flow_graph.add_node(current_name, instructions=[])
                                control_flow_graph.add_edge(prev_name, current_name)
                                lookup_table[instr_target] = 1
                            max_node = max_node if max_node > current_name else current_name
                            max_node += 1
                            prev_name = current_name
                            current_name = max_node
                            control_flow_graph.add_node(current_name, instructions=[])
                            control_flow_graph.add_edge(prev_name, current_name)

                    elif instr.mnemonic in return_functions \
                            or instr.mnemonic == 'mov' and instr.op_str == 'pc, lr' \
                            or instr.mnemonic == 'orr' and instr.op_str == 'r15, r14, r14' \
                            or instr.mnemonic == 'pop' and 'pc' in instr.op_str:
                        if instr.address + instr.size in lookup_table and lookup_table[instr.address + instr.size] == 1:
                            for node in control_flow_graph.nodes:
                                end_for = False
                                if not control_flow_graph.nodes[node]['instructions']:
                                    control_flow_graph.add_edge(current_name, node)
                                else:
                                    for node_instr in control_flow_graph.nodes[node]['instructions']:
                                        if node_instr['address'] == instr.address + instr.size:
                                            control_flow_graph.add_edge(current_name, node)
                                            current_name = node
                                            end_for = True
                                            break
                                    if end_for:
                                        break
                        else:
                            max_node = max_node if max_node > current_name else current_name
                            max_node += 1
                            current_name = max_node

                    if current_name not in control_flow_graph:
                        control_flow_graph.add_node(current_name, instructions=[])
                    control_flow_graph.nodes[current_name]['instructions'].append(
                        {'address': instr.address, 'mnemonic': instr.mnemonic, 'op_str': instr.op_str})
                    lookup_table[instr.address] = 1
            if verbose:
                file_out.close()

            if display_graph:
                nx.draw_networkx(control_flow_graph, with_labels=True, pos=nx.spring_layout(control_flow_graph))
                plt.title(name)
                plt.show()
    return control_flow_graph


def fixUpxPacked(filePath, verbose=True):
    """
    Attempts to fix a UPX-packed binary file by modifying its packed headers and then unpacking it using UPX.

    This function reads the contents of a given file, identifies the UPX packed sections, patches the packed
    headers to correct possible issues, and then uses UPX to unpack the file. The fixed file is saved with
    a "_fixed" suffix.

    :param filePath: Path to the UPX-packed binary file.
    :param verbose: If True, prints detailed debug information.
    :return: True if the file was successfully fixed and unpacked, False otherwise.
    """
    # https://www.akamai.com/blog/security/upx-packed-headaches
    with open(filePath, "rb") as fin:
        content = fin.read()
        magic_pos = content.find(b"UPX!")
        if verbose:
            print(f"upx magic_pos: {magic_pos}")
        if magic_pos <= 0 or magic_pos + 20 >= len(content):
            return False
        p_info_pos = magic_pos + 8
        p_info = content[p_info_pos:p_info_pos + 12]
        if verbose:
            print(f"upx p_info = {str(p_info)}")
        magic_pos_2 = content.find(b"UPX!", magic_pos + 1)
        if magic_pos_2 <= 0:
            return False
        magic_pos_3 = content.find(b"UPX!", magic_pos_2 + 1)
        if magic_pos_3 <= 0:
            return False
        p_filesize_pos = magic_pos_3 + 24
        magic_pos_4 = content.find(b"UPX!", magic_pos_3 + 1)
        if magic_pos_4 > 0:
            p_filesize_pos = magic_pos_4 + 24
        if p_filesize_pos + 4 > len(content):
            return False
        p_filesize = struct.unpack("I", content[p_filesize_pos:p_filesize_pos + 4])[0]
        if verbose:
            print(f"upx p_filesize = {p_filesize}")
        if p_filesize > 10 * 1024 * 1024:
            # too big, not plausible
            return False
        patched_p_info = p_info[:4] + content[p_filesize_pos:p_filesize_pos + 4] * 2
        fixedName = filePath + "_fixed"
        with open(fixedName, "wb") as fout:
            fout.write(content[:p_info_pos])
            fout.write(patched_p_info)
            fout.write(content[p_info_pos + 12:])
        p = subprocess.Popen(["upx", "-d", fixedName], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        out, _err = p.communicate()
        if not out.strip().endswith(b"Unpacked 1 file."):
            os.remove(fixedName)
            return False

    return True


def extract_user_code(filePath, verbose=False):
    """
    Extracts executable sections from an ELF binary file.

    :param filePath: Path to the ELF binary file.
    :param verbose: Whether to print verbose output.
    :return: Tuple containing user code information, ELF identifier, and ELF machine type.
    """
    with open(filePath, 'rb') as file:
        elf_ident = file.read(16)
        if elf_ident[0] != 0x7f or b'ELF' != elf_ident[1:4]:
            return None, None, None
        if elf_ident[5] == 1:
            endianness = '<'
        else:
            endianness = '>'
        try:
            elf_type, elf_machine, elf_version = struct.unpack('{}HHI'.format(endianness), file.read(8))
            if elf_ident[4] == 1:
                elf_entry, elf_prog_header_offset, elf_section_header_offset, elf_flags = struct.unpack(
                    '{}IIII'.format(endianness), file.read(16))
            else:
                elf_entry, elf_prog_header_offset, elf_section_header_offset, elf_flags = struct.unpack(
                    '{}QQQI'.format(endianness), file.read(28))

            elf_file_header_size, elf_prog_header_size, elf_prog_header_entries_num = struct.unpack(
                '{}HHH'.format(endianness), file.read(6))
            elf_section_header_size, elf_section_header_entries_num, elf_section_header_strndx = struct.unpack(
                '{}HHH'.format(endianness), file.read(6))
            if verbose:
                print("Ident: " + str(elf_ident))
                print("Elf Type:" + str(elf_type))
                print("Elf Machine: " + str(elf_machine))
                print("Elf Version: " + str(elf_version))
                print("Elf Entry: " + str(elf_entry))
                print("Elf PH Offset: " + str(elf_prog_header_offset))
                print("Elf SH Offset: " + str(elf_section_header_offset))
                print("Elf Flags: " + str(elf_flags))
                print("Elf File Header Size: " + str(elf_file_header_size))
                print("Elf PH Size: " + str(elf_prog_header_size))
                print("Elf PH entries: " + str(elf_prog_header_entries_num))
                print("Elf SH Size: " + str(elf_section_header_size))
                print("Elf SH entries: " + str(elf_section_header_entries_num))
                print("Elf SH strndx:" + str(elf_section_header_strndx))
        except struct.error:
            return None, None, None

        if elf_section_header_entries_num == 0:
            if fixUpxPacked(filePath, verbose=verbose):
                return extract_user_code(filePath + "_fixed", verbose=verbose)

        file.seek(elf_section_header_offset)

        user_code_info = []
        for i in range(elf_section_header_entries_num):
            if elf_ident[4] == 1:
                try:
                    sh_name, sh_type, sh_flags, sh_addr, sh_offset, sh_size, sh_link, sh_info, sh_addralign, \
                        sh_entsize = struct.unpack('{}IIIIIIIIII'.format(endianness), file.read(40))
                except struct.error:
                    file.seek(elf_section_header_offset + (i + 1) * 40)
                    continue
            else:
                try:
                    sh_name, sh_type, sh_flags, sh_addr, sh_offset, sh_size, sh_link, sh_info, sh_addralign, \
                        sh_entsize = struct.unpack('{}IIQQQQIIQQ'.format(endianness), file.read(64))
                except struct.error:
                    file.seek(elf_section_header_offset + (i + 1) * 64)
                    continue
            if verbose:
                print("Section name: " + str(sh_name))
                print("Section type: " + str(sh_type))
                print("Section flags: " + str(sh_flags))
                print("Virtual Address: " + str(sh_addr))
                print("Offset: " + str(sh_offset))
                print("Size: " + str(sh_size))
                print("Index of associated section: " + str(sh_link))
                print("Extra info: " + str(sh_info))
                print("Alignment: " + str(sh_addralign))
                print("Section entry size: " + str(sh_entsize))
            if sh_flags & 0x4 and sh_size != 0:
                user_code_info.append((sh_offset, sh_addr, sh_size))
        file.close()
        return user_code_info, elf_ident, elf_machine


def node_embedding(graph, dimensions=64, walk_length=80, num_walks=100, p=1, q=1, plot_embedding=False):
    """
    Computes node embeddings using the Node2Vec algorithm.

    :param graph: NetworkX graph.
    :param dimensions: Embedding dimensions.
    :param walk_length: Number of nodes in each walk.
    :param num_walks: Number of walks per node.
    :param p: Return hyper parameter.
    :param q: Inout parameter.
    :param plot_embedding: Whether to plot the embedding.
    :return: Node2Vec model.
    """
    node2vec = Node2Vec(graph, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, p=p, q=q,
                        workers=WORKERS, seed=42)
    model = node2vec.fit(window=10, min_count=1, epochs=10, seed=42)

    node_emb = np.array([model.wv.get_vector(str(node)) for node in graph.nodes()])
    if plot_embedding:
        tsne = TSNE(n_components=2, random_state=42, perplexity=15)
        node_emb_tsne = tsne.fit_transform(node_emb)
        plt.scatter(node_emb_tsne[:, 0], node_emb_tsne[:, 1])
        plt.title("2D Representation of Node Embeddings for " + name)
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.show()
    return model


def edge_embedding(graph, model_node_emb, plot_embedding=False):
    """
    Computes edge embeddings using the Hadamard method.

    :param graph: NetworkX graph.
    :param model_node_emb: Node2Vec model for node embeddings.
    :param plot_embedding: Whether to plot the embedding.
    :return: Numpy array of edge embeddings.
    """
    edges_emb_2 = HadamardEmbedder(keyed_vectors=model_node_emb.wv)
    edges_emb = np.array([edges_emb_2[(str(x[0]), str(x[1]))] for x in graph.edges])
    if plot_embedding:
        tsne = TSNE(n_components=2, perplexity=5)
        edges_emb_tsne = tsne.fit_transform(edges_emb)
        plt.scatter(edges_emb_tsne[:, 0], edges_emb_tsne[:, 1])
        plt.title("2D Representation of Edge Embeddings for " + name)
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.show()
    return edges_emb


def graph_embedding(graph, dimensions=64, plot_embedding=False):
    """
    Computes graph embeddings using the Graph2Vec algorithm.

    :param graph: NetworkX graph.
    :param dimensions: Embedding dimensions.
    :param plot_embedding: Whether to plot the embedding.
    :return: Numpy array of graph embeddings.
    """
    sub_graph = [graph.subgraph(c).copy() for c in nx.weakly_connected_components(graph)]
    for i, s_graph in enumerate(sub_graph):
        sub_graph[i] = nx.convert_node_labels_to_integers(s_graph)
    graph_model = Graph2Vec(dimensions=dimensions, min_count=1, workers=WORKERS)
    graph_model.fit(sub_graph)
    graph_embeddings = graph_model.get_embedding()
    if plot_embedding:
        tsne = TSNE(n_components=2, perplexity=1)
        graph_emb_tsne = tsne.fit_transform(graph_embeddings)
        plt.scatter(graph_emb_tsne[:, 0], graph_emb_tsne[:, 1])
        plt.title("2D Representation of Graph Embeddings for " + name)
        plt.xlabel("Dimension 1")
        plt.ylabel("Dimension 2")
        plt.show()
    return graph_embeddings


@dataclass
class SimilarityArg:
    """
    A data class that stores arguments required for similarity computation.

    Attributes:
            final_names (list): List containing the names of the files to be compared.
            average_node_embeddings (list): List containing node embeddings for each file.
            average_edge_embeddings (list): List containing edge embeddings for each file.
            average_graph_embeddings (list): List containing graph embeddings for each file.
            directory_name (str): Name of the directory where the results will be saved.
            do_hungarian (bool): Whether to use Hungarian algorithm or Greedy algorithm.
            make_padding (bool): Whether to apply padding in case of size mismatch between matrices
            i (int): Index of current computation.
    """
    final_names: list
    average_node_embeddings: list
    average_edge_embeddings: list
    average_graph_embeddings: list
    directory_name: str
    do_hungarian: bool
    make_padding: bool
    i: int


def load_embeddings(embedding_directory):
    """
    Load precomputed embeddings from a specified directory.

    This function traverses the given directory, finds embedding files, and loads node, edge,
    and graph embeddings into separate lists.

    :param embedding_directory: The directory containing the precomputed embeddings.
    :return: A tuple containing:
        - List of final names corresponding to the embeddings.
        - List of average node embeddings.
        - List of average edge embeddings.
        - List of average graph embeddings.
    """
    embedding_files = []
    for root, _, filenames in os.walk(embedding_directory):
        for filename in filenames:
            if "kaiji" not in filename:
                embedding_files.append(os.path.join(root, filename))
    embedding_files.sort()
    average_node_embeddings = []
    average_edge_embeddings = []
    average_graph_embeddings = []
    final_names = []
    for embedding in embedding_files:
        print(embedding)
        if "node" in embedding:
            node_emb = np.load(embedding)
            average_node_embeddings.append(node_emb)
            final_names.append(embedding.replace(embedding_directory + os.path.sep, "").replace("node", ""))
        elif "edge" in embedding:
            edge_emb = np.load(embedding)
            average_edge_embeddings.append(edge_emb)
        elif "graph" in embedding:
            graph_emb = np.load(embedding)
            average_graph_embeddings.append(graph_emb)
    return final_names, average_node_embeddings, average_edge_embeddings, average_graph_embeddings


def greedy_get_best(similarity_matrix, make_padding=True):
    """
    Greedy algorithm to find the best matching similarities.

    :param similarity_matrix: Similarity matrix.
    :param make_padding: Whether to apply padding for matrix matching.
    """
    best_similarities = []
    num_rows, num_cols = similarity_matrix.shape
    for row in range(num_rows):
        max_index = np.argmax(similarity_matrix[row])
        best_similarities.append(similarity_matrix[row][max_index])
        similarity_matrix[:, max_index] = 0
        if not make_padding:
            if (row + 1) == num_cols:
                break
    return best_similarities


def hungarian_get_best(cost_matrix, make_padding=True):
    """
    The Hungarian algorithm to find the best matching similarities.

    :param cost_matrix: Cost matrix (negative similarities).
    :param make_padding: Whether to apply padding for matrix matching.
    :return: List of the best similarities.
    """
    best_similarities = []
    cost_row, cost_col = cost_matrix.shape
    if make_padding:
        if cost_row > cost_col:
            padding = np.zeros((cost_row, cost_row - cost_col))
            cost_matrix = np.append(cost_matrix, padding, axis=1)
    row_indices, col_indices = linear_sum_assignment(cost_matrix)
    for i in range(len(row_indices)):
        best_similarities.append(-cost_matrix[row_indices[i]][col_indices[i]])
    return best_similarities


def embedding_pipeline(compute_average=True):
    """
    Extract user code from the current file, build its CFG, and compute the embeddings.

    This function extracts the user code, sets up the disassembler, builds the control flow graph (CFG),
    and computes node, edge, and graph embeddings. It can also compute the average embeddings if specified.

    :param compute_average: If True, compute the average embeddings, else return all embeddings.
    :return: A tuple containing:
        - Average or all node embeddings.
        - Average or all edge embeddings.
        - Average or all graph embeddings.
    """
    extracted_user_code, elf_id, elf_m = extract_user_code(name)
    if extracted_user_code is not None and extracted_user_code and elf_id is not None and elf_m is not None:
        set_disassembler(elf_id, elf_m)
        cfg = build_CFG(extracted_user_code)
        model_N2V = node_embedding(cfg)
        node_emb = np.array([model_N2V.wv.get_vector(str(node)) for node in cfg.nodes])
        average_node_embedding = np.mean(node_emb, axis=0) if compute_average else node_emb
        edge_emb = edge_embedding(cfg, model_N2V)
        average_edge_embedding = np.mean(edge_emb, axis=0) if compute_average else edge_emb
        graph_emb = graph_embedding(cfg)
        average_graph_embedding = np.mean(graph_emb, axis=0) if compute_average else graph_emb
        return average_node_embedding, average_edge_embedding, average_graph_embedding
    return None, None, None


def process_directory(directory_name, embedding_directory=None, compute_average=True):
    """
    Process all files in a directory to compute and load embeddings.

    This function traverses the specified directory, computes the embeddings for each file,
    and caches the results. It can also load precomputed embeddings from a cache if available.

    :param directory_name: The directory containing the files to process.
    :param embedding_directory: The directory to store or load precomputed embeddings. If None, a default subdirectory is used.
    :param compute_average: If True, compute the average embeddings, else return all embeddings.
    :return: A tuple containing:
        - List of final names corresponding to the embeddings.
        - List of average node embeddings.
        - List of average edge embeddings.
        - List of average graph embeddings.
    """
    global name
    if embedding_directory is None:
        embedding_directory = os.path.join(directory_name, "computed_embeddings")
    if not os.path.isdir(embedding_directory):
        os.makedirs(embedding_directory)
    names = []
    for root, _, filenames in os.walk(directory_name):
        for filename in filenames:
            ignored = False
            for ext in IGNORED_EXTENSIONS:
                if filename.endswith(ext):
                    ignored = True
                    break
            if not ignored:
                names.append(os.path.join(root, filename))
    names.sort()
    average_node_embeddings = []
    average_edge_embeddings = []
    average_graph_embeddings = []
    final_names = []
    for name in names:
        print(name)
        final_name = name.replace(directory_name + os.path.sep, "")

        cache_path = os.path.join(embedding_directory, os.path.basename(name) + ".npy")
        empty_path = os.path.join(embedding_directory, os.path.basename(name) + ".empty")
        if os.path.isfile(cache_path):
            print("loading from cache")
            avg_node_emb, avg_edge_emb, avg_graph_emb = np.load(cache_path, allow_pickle=True)
        elif os.path.isfile(empty_path):
            print("not loading from cache since no code was extracted")
            avg_node_emb, avg_edge_emb, avg_graph_emb = None, None, None
        else:
            print("computing...")
            avg_node_emb, avg_edge_emb, avg_graph_emb = embedding_pipeline(compute_average=compute_average)
            if avg_node_emb is None or avg_edge_emb is None or avg_graph_emb is None:
                open(empty_path, "wb").close()
            else:
                np.save(cache_path, [avg_node_emb, avg_edge_emb, avg_graph_emb], allow_pickle=True)
        if avg_node_emb is not None and avg_edge_emb is not None and avg_graph_emb is not None:
            average_node_embeddings.append(avg_node_emb)
            average_edge_embeddings.append(avg_edge_emb)
            average_graph_embeddings.append(avg_graph_emb)
            final_names.append(final_name)
    return final_names, average_node_embeddings, average_edge_embeddings, average_graph_embeddings


def compute_similarities(arg: SimilarityArg):
    """
    Compute similarities between embeddings and save the results.

    :param arg: Arguments required for similarity computation.
    """
    print(arg.i)
    with open(os.path.join(arg.directory_name, f"results{arg.i:04d}.csv"), "w") as fout:
        for j in range(arg.i, len(arg.final_names)):
            i_rows, _ = arg.average_node_embeddings[arg.i].shape
            j_rows, _ = arg.average_node_embeddings[j].shape
            if i_rows > j_rows:
                first = arg.i
                second = j
            else:
                first = j
                second = arg.i

            node_sims = np.abs(
                cosine_similarity(arg.average_node_embeddings[first], arg.average_node_embeddings[second]))
            edge_sims = np.abs(
                cosine_similarity(arg.average_edge_embeddings[first], arg.average_edge_embeddings[second]))
            graph_sims = np.abs(
                cosine_similarity(arg.average_graph_embeddings[first], arg.average_graph_embeddings[second]))
            if arg.do_hungarian:
                node_costs = -node_sims
                node_sim = np.mean(hungarian_get_best(node_costs, make_padding=arg.make_padding))

                edge_costs = -edge_sims
                edge_sim = np.mean(hungarian_get_best(edge_costs, make_padding=arg.make_padding))

                graph_costs = -graph_sims
                graph_sim = np.mean(hungarian_get_best(graph_costs, make_padding=arg.make_padding))
            else:
                node_sim = np.mean(greedy_get_best(node_sims, make_padding=arg.make_padding))
                edge_sim = np.mean(greedy_get_best(edge_sims, make_padding=arg.make_padding))
                graph_sim = np.mean(greedy_get_best(graph_sims, make_padding=arg.make_padding))

            line = f"{arg.final_names[arg.i]},{arg.final_names[j]},{node_sim:1.3f},{edge_sim:1.3f},{graph_sim:1.3f}"
            # print(line)
            fout.write(f"{line}\n")


def run_general_tests(directory_name, embedding_directory=None, do_hungarian=False, make_padding=True, load_emb=False):
    """
    Run general tests to compute similarities between embeddings in a directory.

    This function processes the specified directory to compute or load embeddings, and then
    computes the similarities between the embeddings using either the Greedy or Hungarian algorithm.

    :param directory_name: The directory containing the files to process.
    :param embedding_directory: The directory to store or load precomputed embeddings. If None, a default subdirectory is used.
    :param do_hungarian: If True, use the Hungarian algorithm for similarity computation, else use the Greedy algorithm.
    :param make_padding: If True, apply padding in case of size mismatch between matrices.
    :param load_emb: If True, load precomputed embeddings from the cache.
    """
    if load_emb:
        final_names, average_node_embeddings, average_edge_embeddings, average_graph_embeddings = load_embeddings(
            embedding_directory=embedding_directory)
    else:
        final_names, average_node_embeddings, average_edge_embeddings, average_graph_embeddings = process_directory(
            directory_name, embedding_directory=embedding_directory, compute_average=False)

    arguments = [SimilarityArg(final_names, average_node_embeddings, average_edge_embeddings, average_graph_embeddings,
                               directory_name, do_hungarian, make_padding, i) for i in range(len(final_names))]
    random.shuffle(arguments)  # prevent first process from getting the longest work

    with multiprocessing.Pool(12) as p:
        p.map(compute_similarities, arguments)


if __name__ == '__main__':
    run_general_tests(sys.argv[1])
